{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Data Generation\n",
    "\n",
    "This notebook demonstrates how to generate synthetic data for Claire Text Classification Task. \n",
    "\n",
    "The data is generated using Open-AI API and the model used is `gpt-3.5-turbo-0125`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Local Environment\n",
    "\n",
    "Install the following packages to run the code locally.\n",
    "\n",
    "```bash\n",
    "pip install openai==1.23.2\n",
    "pip install python-dotenv==1.0.1\n",
    "```\n",
    "\n",
    "Make sure to fill the value of `OPENAI_API_KEY` in `.env` file before running the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "import os \n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the prompt and function to generate synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the API key from the .env file\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize the OpenAI client\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt: str, seed: int = 0):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-0125\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        response_format={ \"type\": \"json_object\" },\n",
    "        seed=seed\n",
    "    )\n",
    "    return response.choices[0].message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"\"\"Text Classification Dataset Creation\n",
    "\n",
    "Generate a JSON-format text classification dataset with entries comprising \"text\" and \"label\" keys.\n",
    "\n",
    "### Be innovative while coming up with examples and think of real-world scenarios.\n",
    "###  I will use the same prompt for all the examples so make sure that you are not repeating anything.\n",
    "\n",
    "Clare, our Conversational AI, interacts with users via WhatsApp and calls to offer personalized mental health support. In WhatsApp conversations, Clare distinguishes between:\n",
    "\n",
    "1. Regular Conversations: Mental health discussions, exercises, etc.\n",
    "2. Product-related Conversations: Queries about Clare's functionalities, communication methods (phone or WhatsApp), etc.\n",
    "3. Subscription-related Conversations: Questions regarding user data retrieval, such as trial duration.\n",
    "4. Suicide: Detection of user expressions indicating active contemplation or planning, requiring redirection to clinically-approved conversational protocols.\n",
    "5. Non-mental Health Topics: Gently guiding conversations back to mental health when users delve into unrelated topics like Clare's movie preferences.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Synthetic Data and Parse the Output into JSON format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check if it works or not \n",
    "output = generate_text(PROMPT, seed=1)\n",
    "json.loads(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CALLS = 100 # Number of Calls to the OpenAI API aka Number of examples per class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_outputs = [] # Stores raw output returned from OpenAI API calls\n",
    "json_objects = [] # Stores properly formatted json strings\n",
    "failed_indexes = [] # Stores indexes that aren't properly formatted json strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num in range(NUM_CALLS):\n",
    "    output = generate_text(PROMPT, seed=num)\n",
    "    raw_outputs.append(output.content)\n",
    "    try:\n",
    "        json_object = json.loads(output.content)\n",
    "        json_objects.append(json_object)\n",
    "    except Exception as e:\n",
    "        failed_indexes.append(num)\n",
    "        print(f\"Error processing output {num}\")\n",
    "    sleep(2) # Sleep for 2 seconds to avoid rate limiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's extract the data sample from the JSON object\n",
    "data_samples = []\n",
    "for obj in json_objects:\n",
    "    for key in obj.keys():\n",
    "        data_samples.append(obj[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing JSON data 0\n",
      "Processing JSON data 1\n",
      "Processing JSON data 2\n",
      "Processing JSON data 3\n",
      "Processing JSON data 4\n",
      "Processing JSON data 5\n",
      "Processing JSON data 6\n",
      "Processing JSON data 7\n",
      "Processing JSON data 8\n",
      "Processing JSON data 9\n",
      "Processing JSON data 10\n",
      "Processing JSON data 11\n",
      "Processing JSON data 12\n",
      "Processing JSON data 13\n",
      "Processing JSON data 14\n",
      "Processing JSON data 15\n",
      "Processing JSON data 16\n",
      "Processing JSON data 17\n",
      "Processing JSON data 18\n",
      "Processing JSON data 19\n",
      "Processing JSON data 20\n",
      "Processing JSON data 21\n",
      "Processing JSON data 22\n",
      "Processing JSON data 23\n",
      "Processing JSON data 24\n",
      "Processing JSON data 25\n",
      "Processing JSON data 26\n",
      "Processing JSON data 27\n",
      "Processing JSON data 28\n",
      "Processing JSON data 29\n",
      "Processing JSON data 30\n",
      "Processing JSON data 31\n",
      "Processing JSON data 32\n",
      "Processing JSON data 33\n",
      "Processing JSON data 34\n",
      "Processing JSON data 35\n",
      "Processing JSON data 36\n",
      "Processing JSON data 37\n",
      "Processing JSON data 38\n",
      "Processing JSON data 39\n",
      "Processing JSON data 40\n",
      "Processing JSON data 41\n",
      "Processing JSON data 42\n",
      "Processing JSON data 43\n",
      "Processing JSON data 44\n",
      "Processing JSON data 45\n",
      "Processing JSON data 46\n",
      "Processing JSON data 47\n",
      "Processing JSON data 48\n",
      "Processing JSON data 49\n",
      "Processing JSON data 50\n",
      "Processing JSON data 51\n",
      "Processing JSON data 52\n",
      "Processing JSON data 53\n",
      "Processing JSON data 54\n",
      "Processing JSON data 55\n",
      "Processing JSON data 56\n",
      "Processing JSON data 57\n",
      "Processing JSON data 58\n",
      "Processing JSON data 59\n",
      "Processing JSON data 60\n",
      "Processing JSON data 61\n",
      "Processing JSON data 62\n",
      "Processing JSON data 63\n",
      "Processing JSON data 64\n",
      "Processing JSON data 65\n",
      "Processing JSON data 66\n",
      "Processing JSON data 67\n",
      "Processing JSON data 68\n",
      "Processing JSON data 69\n",
      "Processing JSON data 70\n",
      "Processing JSON data 71\n",
      "Processing JSON data 72\n",
      "Processing JSON data 73\n",
      "Processing JSON data 74\n",
      "Processing JSON data 75\n",
      "Processing JSON data 76\n",
      "Processing JSON data 77\n",
      "Processing JSON data 78\n",
      "Processing JSON data 79\n",
      "Processing JSON data 80\n",
      "Processing JSON data 81\n",
      "Processing JSON data 82\n",
      "Processing JSON data 83\n",
      "Processing JSON data 84\n",
      "Processing JSON data 85\n",
      "Processing JSON data 86\n",
      "Processing JSON data 87\n",
      "Processing JSON data 88\n",
      "Processing JSON data 89\n",
      "Processing JSON data 90\n",
      "Processing JSON data 91\n",
      "Processing JSON data 92\n",
      "Processing JSON data 93\n",
      "Processing JSON data 94\n",
      "Processing JSON data 95\n",
      "Processing JSON data 96\n",
      "Processing JSON data 97\n",
      "Processing JSON data 98\n",
      "Processing JSON data 99\n",
      "Processing JSON data 100\n"
     ]
    }
   ],
   "source": [
    "all_df = []\n",
    "for idx, json_data in enumerate(data_samples):\n",
    "    print(f\"Processing JSON data {idx}\")\n",
    "    try:\n",
    "        cur_df = pd.DataFrame(json_data)\n",
    "        all_df.append(cur_df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing JSON data {idx}\")\n",
    "all_df = pd.concat(all_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were some more issues when converting the JSON object to a Pandas DataFrame. \n",
    "We see that the JSON object is not in the correct format to be converted to a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(529, 2)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "Product-related Conversations         109\n",
       "Regular Conversations                 105\n",
       "Subscription-related Conversations    104\n",
       "Suicide                               104\n",
       "Non-mental Health Topics              103\n",
       "Regular Conversation                    1\n",
       "Product-related Conversation            1\n",
       "Subscription-related Conversation       1\n",
       "Non-mental Health Topic                 1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Distribution of the classes\n",
    "all_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that some of the labels are not correct. So we need to fix this issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = all_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary mapping the duplicate labels to their originals\n",
    "duplicate_mapping = {\n",
    "    'Regular Conversation': 'Regular Conversations',\n",
    "    'Product-related Conversation': 'Product-related Conversations',\n",
    "    'Subscription-related Conversation': 'Subscription-related Conversations',\n",
    "    'Non-mental Health Topic': 'Non-mental Health Topics'\n",
    "}\n",
    "\n",
    "# Use map function to replace duplicate labels with their originals\n",
    "new_df['label'] = new_df['label'].map(duplicate_mapping).fillna(new_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "Product-related Conversations         110\n",
       "Regular Conversations                 106\n",
       "Subscription-related Conversations    105\n",
       "Suicide                               104\n",
       "Non-mental Health Topics              104\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(529, 2)"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We managed to generate a dataset of 529 samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if there are  duplicates in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83\n",
      "[\"I've been feeling really down lately and struggling with my emotions.\"\n",
      " \"Hi Clare, I have been feeling really anxious lately and I don't know how to cope.\"\n",
      " \"Hi Clare, I've been feeling really down lately and I'm not sure how to cope with it.\"\n",
      " \"Hi Clare, I've been feeling really down lately and I don't know how to cope.\"\n",
      " 'Hi Clare, how are you today?']\n"
     ]
    }
   ],
   "source": [
    "## Let's take any two examples from the dataset and see the text and label\n",
    "\n",
    "regular_conversations = new_df[new_df['label'] == 'Regular Conversations']\n",
    "print(len(regular_conversations['text'].unique()))\n",
    "print(regular_conversations['text'].unique()[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the samples generated here are very repititive as they all are talking about same thing.\n",
    "\n",
    "So, we need to come up with a better prompt to generate more diverse samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88\n",
      "['Have you watched any good movies lately? I need some recommendations.'\n",
      " \"What's your favorite movie, Clare?\"\n",
      " 'Hey Clare, have you watched any good movies lately?'\n",
      " 'So, Clare, do you like action movies or more of a rom-com person?'\n",
      " 'Do you like watching movies, Clare?']\n"
     ]
    }
   ],
   "source": [
    "non_mental_health_topics = new_df[new_df['label'] == 'Non-mental Health Topics']\n",
    "print(len(non_mental_health_topics['text'].unique()))\n",
    "print(non_mental_health_topics['text'].unique()[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the samples generated here are very repititive as they all are talking about movies.\n",
    "\n",
    "So, we need to come up with a better prompt to generate more diverse samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n",
      "[\"I don't see a way out of this darkness. I need help.\"\n",
      " \"I don't see the point in living anymore. I feel like ending everything.\"\n",
      " \"I'm feeling so overwhelmed and I can't see a way out.\"\n",
      " \"I'm feeling like there's no point in going on anymore...\"\n",
      " \"I can't bear this pain anymore, Clare. Everything feels pointless.\"]\n"
     ]
    }
   ],
   "source": [
    "suicide_conversations = new_df[new_df['label'] == \"Suicide\"]\n",
    "print(len(suicide_conversations['text'].unique()))\n",
    "print(suicide_conversations['text'].unique()[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the conversations indicate suicidal thoughts.\n",
    "\n",
    "However, it feels also a bit exaggerated to call all of these conversations suicidal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['How do I schedule a call with Clare? I prefer to talk over the phone.'\n",
      " 'How can I schedule a call with Clare to discuss my mental health concerns?'\n",
      " \"How can I change my notification settings in Clare's app?\"\n",
      " 'How do I change my notification settings on Clare?'\n",
      " 'Does Clare have a feature that allows me to set reminders for self-care activities?']\n"
     ]
    }
   ],
   "source": [
    "product_conversations = new_df[new_df['label'] == \"Product-related Conversations\"]\n",
    "len(product_conversations['text'].unique())\n",
    "print(product_conversations['text'].unique()[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that they all are related to product related."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105\n",
      "['Can I download all my chat data for analysis purposes?'\n",
      " 'Can you provide me with details about the subscription plans available for user data access?'\n",
      " \"I want to know how long the free trial period is for Clare's services.\"\n",
      " \"Can I cancel my subscription to Clare's service? I no longer need it.\"\n",
      " 'How can I access my data after my trial period ends?']\n"
     ]
    }
   ],
   "source": [
    "subscription_conversations = new_df[new_df['label'] == 'Subscription-related Conversations']\n",
    "print(len(subscription_conversations['text'].unique()))\n",
    "print(subscription_conversations['text'].unique()[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that they all are related to subscription related."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the final dataset to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_texts_df = new_df.drop_duplicates(subset=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_texts_df.to_csv('claire_text_classification_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the dataset to Hugging Face \n",
    "\n",
    "\n",
    "You can find the final dataset on Hugging Face Datasets [here](https://huggingface.co/datasets/shub-kris/claire-dataset).\n",
    "\n",
    "I haven't included the code to upload the dataset to Hugging Face Hub to keep the notebook clean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What can be improved?\n",
    "\n",
    "I showed a simple example of generating synthetic data using OpenAI API. There are many ways to improve the quality of the generated data. Some of the ways are:\n",
    "\n",
    "- The quality of the generated data can be improved by using different techniques like using more complex prompts, playing with the temperature parameter, etc.\n",
    "- We can use different models to generate synthetic data. E.g. GPT-4 as it is the latest model.\n",
    "- We can use different prompts to generate synthetic data.\n",
    "- We can generate more synthetic data by increasing the number of iterations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: I ran the code multiple times to generate the data. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "claire-env-3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
