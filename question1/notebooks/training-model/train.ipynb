{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Text Classification Model with DistilBERT and Hugging Face Transformers\n",
    "\n",
    "In this notebook, we will train a text classification model using the [DistilBERT](https://huggingface.co/distilbert/distilbert-base-uncased) model and the Hugging Face Transformers library. We will use the `transformers` library from Hugging Face to fine-tune a pre-trained DistilBERT model for text classification. \n",
    "\n",
    "We will be using the [distilert/distilbert-base-uncased](https://huggingface.co/distilbert/distilbert-base-uncased) version of BERT for this task.\n",
    "\n",
    "The notebook is divided into the following sections:\n",
    "1. Setup Development Environment\n",
    "2. Load Dataset and Preprocess\n",
    "3. Train DistilBERT Model for Text Classification\n",
    "4. Inference on New Data\n",
    "5. Push Model to Hugging Face (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Development Environment\n",
    "\n",
    "Install the following libraries:\n",
    "```bash\n",
    "pip install torch torchvision torchaudio\n",
    "pip install transformers==4.38.2\n",
    "pip install accelerate==0.29.3\n",
    "pip install datasets==2.19.0\n",
    "pip install evaluate==0.4.1\n",
    "pip install scikit-learn==1.2.2\n",
    "pip install numpy==1.25.2\n",
    "pip install pandas==2.0.3\n",
    "```\n",
    "\n",
    "Authenticate with Hugging Face Hub to push the model to the model hub:\n",
    "```bash\n",
    "huggingface-cli login\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset and Preprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shubhamkrishna/.pyenv/versions/3.10.12/envs/claire-env-3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "claire_dataset = load_dataset('shub-kris/claire-dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Data\n",
    "\n",
    "- Changing Name of Labels\n",
    "- Adding integer labels\n",
    "- Splitting the dataset into train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_labels(row):\n",
    "    temp = row['label'].lower().replace(\"-\", \"_\").split()\n",
    "    row['label'] = '_'.join(temp)\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the labels to lowercase and replace '-' with '_'\n",
    "processed_dataset = claire_dataset.map(change_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique labels and map them to integers\n",
    "unique_labels = set(processed_dataset['train']['label'])\n",
    "label2id = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "id2label = {idx: label for idx, label in enumerate(unique_labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the integer labels to the dataset\n",
    "def add_id_mapping(row):\n",
    "    cat_label = row['label']\n",
    "    row['label'] = label2id[cat_label]\n",
    "    row['cat_label'] = cat_label\n",
    "\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 756/756 [00:00<00:00, 32044.44 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Add the integer labels to the dataset\n",
    "processed_dataset = processed_dataset.map(add_id_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the dataset into train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset, train_size:float = 0.8):\n",
    "\n",
    "    if train_size >= 1 or train_size <= 0:\n",
    "        raise ValueError(\"train_size must be > 0 and < 1\")\n",
    "\n",
    "    labels = dataset[\"train\"][\"cat_label\"]\n",
    "\n",
    "    # Get unique classes and their counts\n",
    "    unique_classes, _ = np.unique(labels, return_counts=True)\n",
    "\n",
    "    # Initialize empty lists to hold the split datasets\n",
    "    train_splits = []\n",
    "    val_splits = []\n",
    "\n",
    "    # Split the dataset for each class\n",
    "    for class_label in unique_classes:\n",
    "        # Get indices of samples belonging to the current classß\n",
    "        class_indices = [i for i, label in enumerate(labels) if label == class_label]\n",
    "        \n",
    "        # Randomly shuffle the indices\n",
    "        np.random.shuffle(class_indices)\n",
    "        \n",
    "        # Split the indices into train and test sets\n",
    "        train_indices, val_indices = train_test_split(class_indices, test_size=1-train_size, random_state=42)\n",
    "        \n",
    "        # Add the split indices to the lists\n",
    "        train_splits.extend(train_indices)\n",
    "        val_splits.extend(val_indices)\n",
    "\n",
    "    # Use the indices to create train and test datasets\n",
    "    train_dataset = processed_dataset[\"train\"].select(train_splits)\n",
    "    val_dataset = processed_dataset[\"train\"].select(val_splits)\n",
    "\n",
    "    train_dataset = train_dataset.shuffle(len(train_dataset))\n",
    "    val_dataset = val_dataset.shuffle(len(val_dataset))\n",
    "\n",
    "    return train_dataset, val_dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "train_dataset, val_dataset = split_dataset(processed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size:  0.7976190476190477\n",
      "Validation dataset size:  0.20238095238095238\n"
     ]
    }
   ],
   "source": [
    "## Let's verify the split\n",
    "print(\"Train dataset size: \", len(train_dataset) / len(processed_dataset[\"train\"])) # Should be close to 0.8\n",
    "print(\"Validation dataset size: \", len(val_dataset) / len(processed_dataset[\"train\"])) # Should be close to 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train DistilBERT Model for Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"distilbert/distilbert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## In order to train the model, we need to tokenize the text data \n",
    "# and convert it to a format that the model can understand\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function tokenizes the text data\n",
    "# Tokenization is the process of converting text data into numbers\n",
    "# Max_length is the maximum number of tokens in the input sequence\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 603/603 [00:00<00:00, 47391.05 examples/s]\n",
      "Map: 100%|██████████| 153/153 [00:00<00:00, 31065.91 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Remove the columns that are not needed\n",
    "tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True, remove_columns=[\"cat_label\", \"text\"])\n",
    "tokenized_val_dataset = val_dataset.map(preprocess_function, batched=True, remove_columns=[\"cat_label\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# This data collator will pad the input data so that all samples have the same length\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the metric for evaluation\n",
    "\n",
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "precision = evaluate.load(\"precision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the evaluation function\n",
    "# During evaluation, we will compute accuracy, precision and recall\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    acc = accuracy.compute(predictions=predictions, references=labels)\n",
    "    prec = precision.compute(predictions=predictions, references=labels, average='weighted')\n",
    "    rec = precision.compute(predictions=predictions, references=labels, average='weighted')\n",
    "    return {\"accuracy\": acc, \"precision\": prec, \"recall\": rec}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_id, \n",
    "    num_labels=5, \n",
    "    id2label=id2label, \n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the training arguments and hyperparameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"claire_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize the Hugging Face Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inference on New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"I feel like dying\",\n",
    "         \"What do you know about Football\",\n",
    "         \"How can I change my notification settings\",\n",
    "         \"I want to quit the subscription\",\n",
    "         \"I am not feeling well today\"]\n",
    "\n",
    "## Initialize the text classification pipeline\n",
    "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Push Model to Hugging Face (Optional)\n",
    "\n",
    "Make sure you have authenticated with the Hugging Face Hub by running:\n",
    "\n",
    "```bash\n",
    "huggingface-cli login\n",
    "```\n",
    "\n",
    "The model that I trained is available [here](https://huggingface.co/shub-kris/claire-text-classification-model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make sure to change the user-name to your username\n",
    "model_name = \"user-name/distilbert-base-uncased-finetuned-claire\"\n",
    "\n",
    "model.push_to_hub(model_name)\n",
    "tokenizer.push_to_hub(model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "claire-env-3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
